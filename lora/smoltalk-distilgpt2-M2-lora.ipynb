{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#Â https://huggingface.co/datasets/HuggingFaceTB/smoltalk\n",
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk\", 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/distilbert/distilgpt2\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "\n",
    "def chatml_tokenize(batch):\n",
    "    texts = []\n",
    "    for messages in batch[\"messages\"]:\n",
    "        chat = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                chat += \"<|user|> \" + msg[\"content\"].strip() + \" \" + tokenizer.eos_token + \" \"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                chat += \"<|assistant|> \" + msg[\"content\"].strip() + \" \" + tokenizer.eos_token + \" \"\n",
    "        texts.append(chat.strip())\n",
    "    return tokenizer(texts, padding=False, truncation=False)\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(\n",
    "    chatml_tokenize, batched=True, batch_size=1000, num_proc=num_proc, remove_columns=[\"messages\"]\n",
    ")\n",
    "tokenized_test = dataset[\"test\"].map(\n",
    "    chatml_tokenize, batched=True, batch_size=1000, num_proc=num_proc, remove_columns=[\"messages\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens\n",
    "special_tokens = [\"<|user|>\", \"<|assistant|>\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "\n",
    "# Full device migration for resize operation\n",
    "model = model.to(\"cpu\")  # Move entire model to CPU\n",
    "\n",
    "# Perform resize on CPU\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move back to original device\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify\n",
    "print(f\"Embeddings device: {model.get_input_embeddings().weight.device}\")\n",
    "print(f\"New vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate WITHOUT ChatML formatting\n",
    "def base_model_eval(question):\n",
    "    encoded = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "    generated = model.generate(**encoded, max_new_tokens=20)\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"BEFORE TRAINING (Raw model):\")\n",
    "print(base_model_eval(\"The capital of France is\"))\n",
    "print('\\n...\\n')\n",
    "print(base_model_eval(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# sample random indices from the test set\n",
    "random_indices = random.sample(range(len(tokenized_test)), 50)\n",
    "\n",
    "# create a new Dataset with only those\n",
    "sampled_eval_dataset = tokenized_test.select(random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# r: rank dimension for LoRA update matrices (smaller = more compression)\n",
    "rank_dimension = 6\n",
    "# lora_alpha: scaling factor for LoRA layers (higher = stronger adaptation)\n",
    "lora_alpha = 12\n",
    "# lora_dropout: dropout probability for LoRA layers (helps prevent overfitting)\n",
    "lora_dropout = 0.05\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=rank_dimension,  # Rank dimension - typically between 4-32\n",
    "    lora_alpha=lora_alpha,  # LoRA scaling factor - typically 2x rank\n",
    "    lora_dropout=lora_dropout,  # Dropout probability for LoRA layers\n",
    "    # bias=\"none\",  # Bias type for LoRA. the corresponding biases will be updated during training.\n",
    "    bias=\"lora_only\",\n",
    "    target_modules=\"all-linear\",  # Which modules to apply LoRA to\n",
    "    task_type=\"CAUSAL_LM\",  # Task type for model architecture\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Memory optimization setup\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./trainer_output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    max_steps=1000,\n",
    "    learning_rate=1e-5,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,  # Keep last 2 checkpoints\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,  # Save every 50 steps\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    dataloader_num_workers=1,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_grad_norm=5, # Gradient clipping to combat exploding gradients\n",
    "    num_train_epochs=1,\n",
    "    run_name=\"m2-lora\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    peft_config=peft_config,  # LoRA configuration\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=sampled_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate WITH ChatML formatting\n",
    "def chatml_eval(question):\n",
    "    formatted_prompt = f\"<|user|> {question} <|assistant|>\"\n",
    "    encoded = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    generated = model.generate(**encoded, max_new_tokens=100)\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nAFTER TRAINING (ChatML-formatted):\")\n",
    "print(chatml_eval(\"The capital of France is\"))\n",
    "print('\\n...\\n')\n",
    "print(chatml_eval(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
