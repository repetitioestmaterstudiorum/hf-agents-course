{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /home/ubuntu/.venv/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.venv/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: trl in /home/ubuntu/.venv/lib/python3.12/site-packages (0.15.2)\n",
      "Requirement already satisfied: torch in /home/ubuntu/.venv/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: wandb in /home/ubuntu/.venv/lib/python3.12/site-packages (0.19.8)\n",
      "Requirement already satisfied: ipykernel in /home/ubuntu/.venv/lib/python3.12/site-packages (6.29.5)\n",
      "Requirement already satisfied: huggingface_hub[cli] in /home/ubuntu/.venv/lib/python3.12/site-packages (0.29.2)\n",
      "Requirement already satisfied: filelock in /home/ubuntu/.venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (3.17.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (2024.12.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/ubuntu/.venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (6.0.2)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (4.12.2)\n",
      "Requirement already satisfied: InquirerPy==0.3.4 in /home/ubuntu/.venv/lib/python3.12/site-packages (from huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: pfzy<0.4.0,>=0.3.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (0.3.4)\n",
      "Requirement already satisfied: prompt-toolkit<4.0.0,>=3.0.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from InquirerPy==0.3.4->huggingface_hub[cli]) (3.0.50)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ubuntu/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.venv/lib/python3.12/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.venv/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: accelerate>=0.34.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from trl) (1.4.0)\n",
      "Requirement already satisfied: rich in /home/ubuntu/.venv/lib/python3.12/site-packages (from trl) (13.9.4)\n",
      "Requirement already satisfied: networkx in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (76.0.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: click!=8.0.0,>=7.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from wandb) (8.1.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from wandb) (3.1.44)\n",
      "Requirement already satisfied: platformdirs in /home/ubuntu/.venv/lib/python3.12/site-packages (from wandb) (4.3.6)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from wandb) (5.29.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from wandb) (7.0.0)\n",
      "Requirement already satisfied: pydantic<3,>=2.6 in /home/ubuntu/.venv/lib/python3.12/site-packages (from wandb) (2.10.6)\n",
      "Requirement already satisfied: sentry-sdk>=2.0.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from wandb) (2.22.0)\n",
      "Requirement already satisfied: setproctitle in /home/ubuntu/.venv/lib/python3.12/site-packages (from wandb) (1.3.5)\n",
      "Requirement already satisfied: comm>=0.1.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipykernel) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipykernel) (1.8.13)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipykernel) (9.0.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipykernel) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipykernel) (26.2.1)\n",
      "Requirement already satisfied: tornado>=6.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipykernel) (6.4.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: six>=1.4.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
      "Requirement already satisfied: decorator in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (4.9.0)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (2.19.1)\n",
      "Requirement already satisfied: stack_data in /home/ubuntu/.venv/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ubuntu/.venv/lib/python3.12/site-packages (from jupyter-client>=6.1.12->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /home/ubuntu/.venv/lib/python3.12/site-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.venv/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/.venv/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/.venv/lib/python3.12/site-packages (from requests->huggingface_hub[cli]) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ubuntu/.venv/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from rich->trl) (3.0.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /home/ubuntu/.venv/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.4)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/ubuntu/.venv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/ubuntu/.venv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/ubuntu/.venv/lib/python3.12/site-packages (from prompt-toolkit<4.0.0,>=3.0.1->InquirerPy==0.3.4->huggingface_hub[cli]) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/ubuntu/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /home/ubuntu/.venv/lib/python3.12/site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"huggingface_hub[cli]\" datasets transformers trl torch wandb ipykernel ipywidget iprogress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33membereagle\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# https://huggingface.co/datasets/HuggingFaceTB/smoltalk\n",
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk\", 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages', 'source'],\n",
       "        num_rows: 1043917\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages', 'source'],\n",
       "        num_rows: 54948\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [{'content': 'The function \\\\( g(x) \\\\) satisfies the functional equation\\n\\\\[ g(x + y) = g(x) + g(y) \\\\]\\nfor all real numbers \\\\( x \\\\) and \\\\( y \\\\), and it is given that \\\\( g(3) = 4 \\\\). Find \\\\( g(10) \\\\).',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Given the functional equation and the specific value \\\\( g(3) = 4 \\\\), we can find \\\\( g(1) \\\\) by using the equation multiple times:\\n\\\\[\\ng(3) = g(2) + g(1)\\n\\\\]\\n\\\\[\\ng(2) = g(1) + g(1) = 2g(1)\\n\\\\]\\nThus,\\n\\\\[\\n4 = 2g(1) + g(1) = 3g(1)\\n\\\\]\\n\\\\[\\ng(1) = \\\\frac{4}{3}\\n\\\\]\\nNow we can find \\\\( g(10) \\\\) using \\\\( g(1) \\\\):\\n\\\\[\\ng(10) = 10g(1) = 10 \\\\times \\\\frac{4}{3} = \\\\frac{40}{3}\\n\\\\]\\nHence, the value of \\\\( g(10) \\\\) is \\\\(\\\\boxed{\\\\frac{40}{3}}\\\\).',\n",
       "   'role': 'assistant'}],\n",
       " 'source': 'numina-cot-100k'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/microsoft/DialoGPT-medium\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# https://huggingface.co/Qwen/Qwen2.5-0.5B\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\", use_fast=True)\n",
    "\n",
    "# https://huggingface.co/distilbert/distilgpt2\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "\n",
    "def chatml_tokenize(batch):\n",
    "    texts = []\n",
    "    for messages in batch[\"messages\"]:\n",
    "        chat = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                chat += \"<|user|> \" + msg[\"content\"].strip() + \" \" + tokenizer.eos_token + \" \"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                chat += \"<|assistant|> \" + msg[\"content\"].strip() + \" \" + tokenizer.eos_token + \" \"\n",
    "        texts.append(chat.strip())\n",
    "    return tokenizer(texts, padding=False, truncation=False)\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(\n",
    "    chatml_tokenize, batched=True, batch_size=1000, num_proc=num_proc, remove_columns=[\"messages\"]\n",
    ")\n",
    "tokenized_test = dataset[\"test\"].map(\n",
    "    chatml_tokenize, batched=True, batch_size=1000, num_proc=num_proc, remove_columns=[\"messages\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should show ChatML-formatted text\n",
    "print(\"Sample training example:\")\n",
    "print(tokenizer.decode(tokenized_train[0][\"input_ids\"])) \n",
    "# Expected: \"<|user|> ... <|assistant|> ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add special tokens to tokenizer\n",
    "special_tokens = [\"<|user|>\", \"<|assistant|>\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is ÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂÂ\n",
      "What is the capital of France?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate WITHOUT ChatML formatting\n",
    "def base_model_eval(question):\n",
    "    encoded = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "    generated = model.generate(**encoded, max_new_tokens=20)\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"BEFORE TRAINING (Raw model):\")\n",
    "print(base_model_eval(\"The capital of France is\"))\n",
    "print(base_model_eval(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# sample random indices from the test set\n",
    "random_indices = random.sample(range(len(tokenized_test)), 400)\n",
    "\n",
    "# create a new Dataset with only those\n",
    "sampled_eval_dataset = tokenized_test.select(random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The trainer crashes with wandb active, even in offline mode .. login worked (above). The quick fix is to deactivate wandb\n",
    "\n",
    "import os\n",
    "# os.environ[\"WANDB_MODE\"] = \"offline\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/ubuntu/.venv/lib/python3.12/site-packages/torch/cuda/__init__.py:734: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='51' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  51/1000 04:48 < 1:33:17, 0.17 it/s, Epoch 0.00/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='29' max='6869' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  29/6869 01:24 < 5:42:56, 0.33 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Enable gradient checkpointing to reduce memory\n",
    "# model.gradient_checkpointing_enable()\n",
    "# model.config.use_cache = False\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Use EOS token as padding if not set\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./trainer_output\",\n",
    "    per_device_train_batch_size=4,\n",
    "    max_steps=50,\n",
    "    learning_rate=1e-5,\n",
    "    bf16=True,\n",
    "    # fp16=True, # backup option\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,  # Keep last 2 checkpoints\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,  # Save every 50 steps\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    dataloader_num_workers=4,\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    run_name=\"infomaniak_gpu\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=sampled_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate WITH ChatML formatting\n",
    "def chatml_eval(question):\n",
    "    formatted_prompt = f\"<|user|> {question} <|assistant|>\"\n",
    "    encoded = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    generated = model.generate(**encoded, max_new_tokens=100)\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nAFTER TRAINING (ChatML-formatted):\")\n",
    "print(chatml_eval(\"The capital of France is\"))\n",
    "print('...')\n",
    "print(chatml_eval(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert model.get_output_embeddings().weight.data_ptr() == model.get_input_embeddings().weight.data_ptr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue training, use last checkpoint\n",
    "trainer.args.max_steps += 1000\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate WITH ChatML formatting\n",
    "def chatml_eval(question):\n",
    "    formatted_prompt = f\"<|user|> {question} <|assistant|>\"\n",
    "    encoded = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    generated = model.generate(**encoded, max_new_tokens=100)\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nAFTER TRAINING (ChatML-formatted):\")\n",
    "print(chatml_eval(\"The capital of France is\"))\n",
    "print('...')\n",
    "print(chatml_eval(\"What is the capital of France?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
