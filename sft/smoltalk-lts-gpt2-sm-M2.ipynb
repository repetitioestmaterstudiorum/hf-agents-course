{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#Â https://huggingface.co/datasets/HuggingFaceTB/smoltalk\n",
    "dataset = load_dataset(\"HuggingFaceTB/smoltalk\", 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# https://huggingface.co/microsoft/DialoGPT-medium\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# https://huggingface.co/Qwen/Qwen2.5-0.5B\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B\", use_fast=True)\n",
    "\n",
    "# https://huggingface.co/distilbert/distilgpt2\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\", use_fast=True)\n",
    "\n",
    "# https://huggingface.co/microsoft/lts-gpt2-sm\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/lts-gpt2-sm\")\n",
    "# version with 8.23M params -> 93.88MB RAM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/lts-gpt2-sm\", subfolder=\"gpt2_538d4b101df48595a935d90dbf4a7fb2ac09ac01\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "num_proc = multiprocessing.cpu_count()\n",
    "\n",
    "def chatml_tokenize(batch):\n",
    "    texts = []\n",
    "    for messages in batch[\"messages\"]:\n",
    "        chat = \"\"\n",
    "        for msg in messages:\n",
    "            if msg[\"role\"] == \"user\":\n",
    "                chat += \"<|user|> \" + msg[\"content\"].strip() + \" \" + tokenizer.eos_token + \" \"\n",
    "            elif msg[\"role\"] == \"assistant\":\n",
    "                chat += \"<|assistant|> \" + msg[\"content\"].strip() + \" \" + tokenizer.eos_token + \" \"\n",
    "        texts.append(chat.strip())\n",
    "    return tokenizer(texts, padding=False, truncation=False)\n",
    "\n",
    "tokenized_train = dataset[\"train\"].map(\n",
    "    chatml_tokenize, batched=True, batch_size=1000, num_proc=num_proc, remove_columns=[\"messages\"]\n",
    ")\n",
    "tokenized_test = dataset[\"test\"].map(\n",
    "    chatml_tokenize, batched=True, batch_size=1000, num_proc=num_proc, remove_columns=[\"messages\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Should show ChatML-formatted text\n",
    "print(\"Sample training example:\")\n",
    "print(tokenizer.decode(tokenized_train[0][\"input_ids\"])) \n",
    "# Expected: \"<|user|> ... <|assistant|> ...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"mps\" if torch.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Add special tokens to tokenizer\n",
    "# special_tokens = [\"<|user|>\", \"<|assistant|>\"]\n",
    "# tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Add special tokens\n",
    "special_tokens = [\"<|user|>\", \"<|assistant|>\"]\n",
    "tokenizer.add_special_tokens({\"additional_special_tokens\": special_tokens})\n",
    "\n",
    "# Full device migration for resize operation\n",
    "model = model.to(\"cpu\")  # Move entire model to CPU\n",
    "\n",
    "# Perform resize on CPU\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Move back to original device\n",
    "model = model.to(device)\n",
    "\n",
    "# Verify\n",
    "print(f\"Embeddings device: {model.get_input_embeddings().weight.device}\")\n",
    "print(f\"New vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate WITHOUT ChatML formatting\n",
    "def base_model_eval(question):\n",
    "    encoded = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "    generated = model.generate(**encoded, max_new_tokens=20)\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"BEFORE TRAINING (Raw model):\")\n",
    "print(base_model_eval(\"The capital of France is\"))\n",
    "print(base_model_eval(\"What is the capital of France?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# sample random indices from the test set\n",
    "random_indices = random.sample(range(len(tokenized_test)), 50)\n",
    "\n",
    "# create a new Dataset with only those\n",
    "sampled_eval_dataset = tokenized_test.select(random_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PYTORCH_MPS_HIGH_WATERMARK_RATIO\"] = \"0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "# Memory optimization setup\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./trainer_output\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=1,\n",
    "    max_steps=len(tokenized_train) // 100,\n",
    "    learning_rate=1e-5,\n",
    "    bf16=True,\n",
    "    logging_steps=10,\n",
    "    save_total_limit=2,  # Keep last 2 checkpoints\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,  # Save every 50 steps\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    dataloader_num_workers=1,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"adafactor\",\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    run_name=\"m2-sm\"\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=sampled_eval_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate WITH ChatML formatting\n",
    "def chatml_eval(question):\n",
    "    formatted_prompt = f\"<|user|> {question} <|assistant|>\"\n",
    "    encoded = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "    generated = model.generate(**encoded, max_new_tokens=100)\n",
    "    return tokenizer.decode(generated[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"\\nAFTER TRAINING (ChatML-formatted):\")\n",
    "print(chatml_eval(\"The capital of France is\"))\n",
    "print('...')\n",
    "print(chatml_eval(\"What is the capital of France?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
